import os
import sys
import numpy as np
import mat73
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Add paths for NOSA v2
sys.path.append('NOSAv2')

# Import NOSA v2
from NOSAv2.NOSA_v2 import predict_tumor

class SegmentationMetrics:
    """Class to calculate various segmentation performance metrics"""
    
    @staticmethod
    def jaccard_index(y_true, y_pred):
        """Calculate Jaccard Index (IoU)"""
        intersection = np.sum(y_true * y_pred)
        union = np.sum(y_true) + np.sum(y_pred) - intersection
        return intersection / (union + 1e-8)
    
    @staticmethod
    def precision_recall_f1(y_true, y_pred):
        """Calculate Precision, Recall, and F1 score"""
        tn, fp, fn, tp = confusion_matrix(y_true.flatten(), y_pred.flatten()).ravel()
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        return precision, recall, f1

class HeatmapComparison:
    """Class to compare NOSA v2 performance with and without heatmap"""
    
    def __init__(self, test_data_dir, results_dir="heatmap_analysis"):
        self.test_data_dir = test_data_dir
        self.results_dir = results_dir
        self.metrics = SegmentationMetrics()
        self.heatmap_threshold = 0.7  # 70% threshold after heatmap application
        
        # Create results directory
        os.makedirs(results_dir, exist_ok=True)
        
        # Initialize results storage
        self.results = {
            'Without_Heatmap': [],
            'With_Heatmap': []
        }
        
    def load_test_files(self):
        """Load all .mat files from test directory"""
        test_files = []
        for file in os.listdir(self.test_data_dir):
            if file.endswith('.mat'):
                file_path = os.path.join(self.test_data_dir, file)
                test_files.append(file_path)
        return test_files
    
    def load_ground_truth(self, file_path):
        """Load ground truth mask from .mat file"""
        try:
            mat = mat73.loadmat(file_path)
            if 'cjdata' in mat and 'tumorMask' in mat['cjdata']:
                return np.array(mat['cjdata']['tumorMask'], dtype=np.uint8)
            else:
                return None
        except:
            return None
    
    def predict_with_heatmap(self, file_path):
        """Predict tumor mask with heatmap applied exactly like NOSAv2 GUI"""
        try:
            # Load heatmap mask
            heatmap_path = os.path.join('NOSAv2', 'tumor_heatmap_mask.npy')
            if not os.path.exists(heatmap_path):
                print(f"Heatmap file not found: {heatmap_path}")
                return None, None
            
            heatmap_mask = np.load(heatmap_path)
            
            # 1. Get initial binary prediction (like NOSAv2 GUI does)
            pred_mask, test_img = predict_tumor(file_path, threshold=0.45)
            
            if pred_mask is None:
                return None, None
            
            # Convert binary mask back to float for heatmap application
            pred_mask = pred_mask.astype(np.float32)
            
            # 2. Resize heatmap to match prediction if needed
            if heatmap_mask.shape != pred_mask.shape:
                from skimage.transform import resize
                heatmap_mask = resize(heatmap_mask, pred_mask.shape, preserve_range=True, anti_aliasing=False)
            
            # 3. Apply heatmap: multiply prediction with heatmap (exactly like NOSAv2 GUI)
            heatmap_adjusted = pred_mask * heatmap_mask
            
            # 4. Normalize to [0,1] again after multiplication (like NOSAv2 GUI)
            heatmap_adjusted = heatmap_adjusted / (heatmap_adjusted.max() + 1e-8)
            
            # 5. Apply 70% threshold (this is the slider value from NOSAv2)
            threshold = 0.70  # 70% like the slider in NOSAv2
            heatmap_pred = (heatmap_adjusted >= threshold).astype(np.uint8)
            
            return heatmap_pred, test_img
        
        except Exception as e:
            print(f"Error applying heatmap to {file_path}: {str(e)}")
            return None, None
    
    def evaluate_single_image(self, file_path):
        """Evaluate a single image with and without heatmap"""
        try:
            # Load ground truth
            gt_mask = self.load_ground_truth(file_path)
            if gt_mask is None:
                return None, None
            
            # Get prediction without heatmap
            pred_no_heatmap, _ = predict_tumor(file_path, threshold=0.45)
            if pred_no_heatmap is None:
                return None, None
            
            # Get prediction with heatmap
            pred_with_heatmap, _ = self.predict_with_heatmap(file_path)
            if pred_with_heatmap is None:
                return None, None
            
            # Ensure same dimensions
            if gt_mask.shape != pred_no_heatmap.shape:
                from skimage.transform import resize
                pred_no_heatmap = resize(pred_no_heatmap, gt_mask.shape, preserve_range=True, anti_aliasing=False)
                pred_no_heatmap = (pred_no_heatmap > 0.5).astype(np.uint8)
                
            if gt_mask.shape != pred_with_heatmap.shape:
                from skimage.transform import resize
                pred_with_heatmap = resize(pred_with_heatmap, gt_mask.shape, preserve_range=True, anti_aliasing=False)
                pred_with_heatmap = (pred_with_heatmap > 0.5).astype(np.uint8)
            
            # Calculate metrics for both predictions
            results_no_heatmap = self.calculate_metrics(gt_mask, pred_no_heatmap, file_path)
            results_with_heatmap = self.calculate_metrics(gt_mask, pred_with_heatmap, file_path)
            
            return results_no_heatmap, results_with_heatmap
            
        except Exception as e:
            print(f"Error evaluating {file_path}: {str(e)}")
            return None, None
    
    def calculate_metrics(self, gt_mask, pred_mask, file_path):
        """Calculate all metrics for a single prediction"""
        precision, recall, f1 = self.metrics.precision_recall_f1(gt_mask, pred_mask)
        jaccard = self.metrics.jaccard_index(gt_mask, pred_mask)
        
        return {
            'file': os.path.basename(file_path),
            'jaccard': jaccard,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def run_comparison(self):
        """Run full comparison with and without heatmap"""
        test_files = self.load_test_files()
        
        if not test_files:
            print("No test files found!")
            return
        
        print(f"Found {len(test_files)} test files")
        print(f"Heatmap threshold: {self.heatmap_threshold*100}%")
        
        # Evaluate all files
        print("\nEvaluating NOSA v2 with and without heatmap...")
        for file_path in tqdm(test_files, desc="Processing"):
            result_no_heatmap, result_with_heatmap = self.evaluate_single_image(file_path)
            
            if result_no_heatmap and result_with_heatmap:
                self.results['Without_Heatmap'].append(result_no_heatmap)
                self.results['With_Heatmap'].append(result_with_heatmap)
    
        print(f"\nSuccessfully evaluated {len(self.results['Without_Heatmap'])} files")
    
    def calculate_summary_statistics(self):
        """Calculate summary statistics for both conditions"""
        summary = {}
        
        for condition_name, results in self.results.items():
            if not results:
                continue
                
            df = pd.DataFrame(results)
            
            summary[condition_name] = {
                'mean_jaccard': df['jaccard'].mean(),
                'std_jaccard': df['jaccard'].std(),
                'mean_f1': df['f1'].mean(),
                'std_f1': df['f1'].std(),
                'mean_precision': df['precision'].mean(),
                'std_precision': df['precision'].std(),
                'mean_recall': df['recall'].mean(),
                'std_recall': df['recall'].std(),
                'sample_count': len(results)
            }
        
        return summary
    
    def create_visualizations(self):
        """Create comparison visualizations"""
        
        # Convert results to DataFrames
        df_no_heatmap = pd.DataFrame(self.results['Without_Heatmap'])
        df_with_heatmap = pd.DataFrame(self.results['With_Heatmap'])
        
        if df_no_heatmap.empty or df_with_heatmap.empty:
            print("No data to visualize!")
            return
        
        # Add condition column
        df_no_heatmap['condition'] = 'Without Heatmap'
        df_with_heatmap['condition'] = 'With Heatmap (70%)'
        
        # Combine DataFrames
        df_combined = pd.concat([df_no_heatmap, df_with_heatmap], ignore_index=True)
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('NOSA v2: Heatmap Impact Analysis (70% Strength)', fontsize=16, fontweight='bold')
        
        # Metrics to plot
        metrics = ['jaccard', 'f1', 'precision', 'recall']
        
        for i, metric in enumerate(metrics):
            row = i // 2
            col = i % 2
            
            # Box plot comparison
            sns.boxplot(data=df_combined, x='condition', y=metric, ax=axes[row, col])
            axes[row, col].set_title(f'{metric.capitalize()} Score Comparison')
            axes[row, col].set_ylabel(f'{metric.capitalize()} Score')
            axes[row, col].tick_params(axis='x', rotation=15)
            
            # Add mean values as text
            no_heatmap_mean = df_no_heatmap[metric].mean()
            with_heatmap_mean = df_with_heatmap[metric].mean()
            axes[row, col].text(0, no_heatmap_mean + 0.02, f'μ={no_heatmap_mean:.3f}', 
                               ha='center', fontweight='bold', color='blue')
            axes[row, col].text(1, with_heatmap_mean + 0.02, f'μ={with_heatmap_mean:.3f}', 
                               ha='center', fontweight='bold', color='orange')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'heatmap_comparison_boxplots.png'), dpi=300, bbox_inches='tight')
        plt.show()
        
        # Create line plot for mean values comparison
        plt.figure(figsize=(10, 6))
        
        # Calculate means for both conditions
        no_heatmap_means = [df_no_heatmap[metric].mean() for metric in metrics]
        with_heatmap_means = [df_with_heatmap[metric].mean() for metric in metrics]
        
        # Create the line plot
        plt.plot(metrics, no_heatmap_means, marker='o', linewidth=3, markersize=8, 
                 color='blue', label='Without Heatmap', markerfacecolor='lightblue', 
                 markeredgecolor='blue', markeredgewidth=2)
        plt.plot(metrics, with_heatmap_means, marker='s', linewidth=3, markersize=8, 
                 color='orange', label='With Heatmap (70%)', markerfacecolor='lightsalmon', 
                 markeredgecolor='orange', markeredgewidth=2)
        
        # Add value labels on points
        for i, (metric, no_hm_val, with_hm_val) in enumerate(zip(metrics, no_heatmap_means, with_heatmap_means)):
            plt.text(i, no_hm_val + 0.02, f'{no_hm_val:.3f}', ha='center', va='bottom', 
                    fontweight='bold', color='blue', fontsize=10)
            plt.text(i, with_hm_val - 0.03, f'{with_hm_val:.3f}', ha='center', va='top', 
                    fontweight='bold', color='orange', fontsize=10)
        
        plt.xlabel('Metrics', fontsize=12, fontweight='bold')
        plt.ylabel('Mean Score', fontsize=12, fontweight='bold')
        plt.title('Heatmap Impact on NOSA v2 Performance (Line Graph)', fontsize=14, fontweight='bold')
        plt.xticks(range(len(metrics)), [metric.capitalize() for metric in metrics])
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1.1)
        
        # Add improvement annotations
        for i, (no_hm_val, with_hm_val) in enumerate(zip(no_heatmap_means, with_heatmap_means)):
            improvement = ((with_hm_val - no_hm_val) / no_hm_val) * 100 if no_hm_val > 0 else 0
            if improvement > 0:
                plt.annotate(f'+{improvement:.1f}%', xy=(i, max(no_hm_val, with_hm_val) + 0.05), 
                            ha='center', fontsize=9, color='green', fontweight='bold')
            elif improvement < 0:
                plt.annotate(f'{improvement:.1f}%', xy=(i, max(no_hm_val, with_hm_val) + 0.05), 
                            ha='center', fontsize=9, color='red', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'heatmap_comparison_line_graph.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_results(self):
        """Save detailed results to CSV files"""
        
        # Save individual results
        if self.results['Without_Heatmap']:
            df_no_heatmap = pd.DataFrame(self.results['Without_Heatmap'])
            df_no_heatmap.to_csv(os.path.join(self.results_dir, 'without_heatmap_results.csv'), index=False)
        
        if self.results['With_Heatmap']:
            df_with_heatmap = pd.DataFrame(self.results['With_Heatmap'])
            df_with_heatmap.to_csv(os.path.join(self.results_dir, 'with_heatmap_results.csv'), index=False)
        
        # Save summary statistics
        summary = self.calculate_summary_statistics()
        summary_df = pd.DataFrame(summary).T
        summary_df.to_csv(os.path.join(self.results_dir, 'heatmap_summary_statistics.csv'))
        
        print(f"\nResults saved to {self.results_dir}/")
        return summary
    
    def print_summary_report(self):
        """Print a formatted summary report"""
        summary = self.calculate_summary_statistics()
        
        print("\n" + "="*70)
        print("NOSA v2 HEATMAP IMPACT ANALYSIS REPORT")
        print("="*70)
        
        if 'Without_Heatmap' in summary and 'With_Heatmap' in summary:
            print(f"\nSample Size: {summary['Without_Heatmap']['sample_count']} images")
            print(f"Heatmap Threshold: {self.heatmap_threshold*100}%")  # Fixed: use heatmap_threshold instead of heatmap_strength
            print("\nMETRIC COMPARISON:")
            print("-" * 70)
            print(f"{'Metric':<15} {'Without Heatmap':<20} {'With Heatmap (70%)':<20} {'Change':<10}")
            print("-" * 70)
            
            metrics = ['jaccard', 'f1', 'precision', 'recall']
            
            for metric in metrics:
                no_hm_val = summary['Without_Heatmap'][f'mean_{metric}']
                with_hm_val = summary['With_Heatmap'][f'mean_{metric}']
                change = ((with_hm_val - no_hm_val) / no_hm_val) * 100 if no_hm_val > 0 else 0
                
                print(f"{metric.capitalize():<15} {no_hm_val:.3f} ± {summary['Without_Heatmap'][f'std_{metric}']:8.3f} "
                      f"{with_hm_val:.3f} ± {summary['With_Heatmap'][f'std_{metric}']:8.3f} "
                      f"{change:+.1f}%")
            
            print("-" * 70)
            
            # Statistical significance test
            f1_no_heatmap = [r['f1'] for r in self.results['Without_Heatmap']]
            f1_with_heatmap = [r['f1'] for r in self.results['With_Heatmap']]
            
            from scipy import stats
            t_stat, p_value = stats.ttest_rel(f1_with_heatmap, f1_no_heatmap)
            
            print(f"\nSTATISTICAL ANALYSIS:")
            print(f"Paired t-test (F1 score): t={t_stat:.3f}, p={p_value:.4f}")
            if p_value < 0.05:
                print("*** Statistically significant difference (p < 0.05)")
            else:
                print("No statistically significant difference (p >= 0.05)")
            
            # Summary interpretation
            print(f"\nSUMMARY:")
            if summary['With_Heatmap']['mean_precision'] > summary['Without_Heatmap']['mean_precision']:
                print("✅ Heatmap improved precision (reduced false positives)")
            else:
                print("⚠️ Heatmap reduced precision (may have affected true positives)")
                
            if summary['With_Heatmap']['mean_recall'] > summary['Without_Heatmap']['mean_recall']:
                print("✅ Heatmap improved recall (better tumor detection)")
            else:
                print("⚠️ Heatmap reduced recall (may have suppressed some true positives)")
                
        print("\n" + "="*70)

def main():
    """Main function to run the heatmap comparison"""
    
    # Configuration
    TEST_DATA_DIR = "C:/Matura/testing"  # Update this path to your test data directory
    RESULTS_DIR = "heatmap_analysis"
    
    # Check if test directory exists
    if not os.path.exists(TEST_DATA_DIR):
        print(f"Test data directory not found: {TEST_DATA_DIR}")
        print("Please update the TEST_DATA_DIR variable to point to your test data.")
        return
    
    # Check if heatmap file exists
    heatmap_path = os.path.join('NOSAv2', 'tumor_heatmap_mask.npy')
    if not os.path.exists(heatmap_path):
        print(f"Heatmap file not found: {heatmap_path}")
        print("Please ensure the tumor_heatmap_mask.npy file is in the NOSAv2 directory.")
        return
    
    # Initialize comparison
    comparison = HeatmapComparison(TEST_DATA_DIR, RESULTS_DIR)
    
    print("Starting NOSA v2 Heatmap Impact Analysis...")
    print(f"Test data directory: {TEST_DATA_DIR}")
    print(f"Results will be saved to: {RESULTS_DIR}")
    print(f"Heatmap strength: 70%")
    
    # Run the comparison
    comparison.run_comparison()
    
    # Generate and save results
    summary = comparison.save_results()
    
    # Create visualizations
    comparison.create_visualizations()
    
    # Print summary report
    comparison.print_summary_report()
    
    print(f"\nHeatmap analysis complete! Check {RESULTS_DIR}/ for detailed results and visualizations.")

if __name__ == "__main__":
    main()